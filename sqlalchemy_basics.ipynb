{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databases with SQLAlchemy\n",
    "\n",
    "from sqlalquemy import create_engine\n",
    "# \"dialect+driver://username:password@host:port/database\"\n",
    "# For Postgresql a good driver is psycopg2 : \"postgresql+psycopg2://\"\n",
    "# For MySQL pymysql : \"mysql+pymysql://\"\n",
    "# For sqlite that connects to the local files databases using python sqlite3 we must pass the path\n",
    "engine = create_engine(\"sqlite:////Users/taymaradias/Downloads/census.sqlite\")\n",
    "print(engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reflecting the database\n",
    "\n",
    "from sqlalquemy import MetaData, Table\n",
    "# Create a metadata object: metadata\n",
    "metadata = MetaData()\n",
    "# Reflect census table from the engine: census\n",
    "census = Table('census', metadata, autoload=True, autoload_with=engine)\n",
    "# Print census table metadata\n",
    "print(repr(census))\n",
    "# Print the column names\n",
    "print(census.columns.keys())\n",
    "# Print full metadata of census\n",
    "print(repr(metadata.tables['census']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying data with sqlalchemy\n",
    "\n",
    "from sqlalchemy import select\n",
    "# Build select statement\n",
    "stmt = select([census])\n",
    "# Execute the statement on connection and fetch 10 records\n",
    "results = connection.execute(stmt).fetchmany(size=10)\n",
    "# Get the first row of the results by using an index: first_row\n",
    "first_row = results[0]\n",
    "print(first_row[0])\n",
    "print(first_row['state'])\n",
    "\n",
    "#Using where clauses and in_ to iterate in a list, note there's no need to fetch the data\n",
    "states = ['New York', 'California', 'Texas']\n",
    "stmt = select([census])\n",
    "stmt = stmt.where(census.columns.state.in_(states))\n",
    "for result in connection.execute(stmt):\n",
    "    print(result.state, result.pop2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using group_by and order_by and case\n",
    "\n",
    "# Build query to return state names by population difference \n",
    "stmt = select([census.columns.state, (census.columns.pop2008-census.columns.pop2000).label(pop_change)])\n",
    "# Append group by for the state\n",
    "stmt = stmt.group_by(census.columns.state)\n",
    "\n",
    "# Append order by for pop_change descendingly and limit to the first 5 elements\n",
    "stmt = stmt.order_by(desc('pop_change')).limit(5)\n",
    "results = connection.execute(stmt).fetchall()\n",
    "for result in results:\n",
    "    print('{}:{}'.format(result.state, result.pop_change))\n",
    "\n",
    "from sqlalchemy import case, cast, Float\n",
    "# Expression to calculate female population in 2000 grouped by state\n",
    "stmt = select([census.columns.state,\n",
    "    (func.sum(\n",
    "        case([\n",
    "            (census.columns.sex == 'F', census.columns.pop2000)\n",
    "        ], else_=0)) /\n",
    "     cast(func.sum(census.columns.pop2000), Float) * 100).label('percent_female')\n",
    "])\n",
    "stmt = stmt.group_by(census.columns.state)\n",
    "results = connection.execute(stmt).fetchall()\n",
    "for result in results:\n",
    "    print(result.state, result.percent_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting more than one table and join\n",
    "\n",
    "stmt = select([census, state_fact])\n",
    "# Add a select_from clause that wraps a join for the census and state_fact\n",
    "# tables where the census state column and state_fact name column match\n",
    "stmt_join = stmt.select_from(\n",
    "    census.join(state_fact, census.columns.state == state_fact.columns.name))\n",
    "result = connection.execute(stmt_join).first()\n",
    "for key in result.keys():\n",
    "    print(key, getattr(result, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make an alias of the tables to work with intern queries \n",
    "\n",
    "managers = employees.alias()\n",
    "stmt = select(\n",
    "    [managers.columns.name.label('manager'),\n",
    "     employees.columns.name.label('employee')]\n",
    ")\n",
    "\n",
    "# Match managers id with employees mgr\n",
    "stmt = stmt.where(managers.columns.id == employees.columns.mgr)\n",
    "stmt = stmt.order_by(managers.columns.name)\n",
    "results = connection.execute(stmt).fetchall()\n",
    "for record in results:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating Table\n",
    "\n",
    "from sqlalchemy import Table, Column, String, Integer, Float, Boolean\n",
    "\n",
    "# Define a new table with columns names and type\n",
    "data = Table('data', metadata,\n",
    "             Column('name', String(255)),\n",
    "             Column('count', Integer()),\n",
    "             Column('amount', Float()),\n",
    "             Column('valid', Boolean())\n",
    ")\n",
    "# Use the metadata to create the table\n",
    "metadata.create_all(engine)\n",
    "#Insert data using insert\n",
    "from sqlalchemy import insert\n",
    "# Build an insert statement to insert a record into the data table\n",
    "insert_stmt = insert(data).values(name=\"Anna\", count=1, amount=1000.00, valid=True)\n",
    "results = connection.execute(insert_stmt)\n",
    "#Or creats a list with dictionaries to insert more than one entry\n",
    "values_list = [\n",
    "    {'name': \"Anna\", 'count': 1, 'amount': 1000.00, 'valid': True},\n",
    "    {'name': \"Taylor\", 'count': 1, 'amount': 750.00, 'valid': False}\n",
    "]\n",
    "stmt = insert(data)\n",
    "results = connection.execute(stmt, values_list)\n",
    "# Print rowcount\n",
    "print(results.rowcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating table from csv with pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "census_df = pd.read_csv(\"census.csv\", header=None)\n",
    "# rename the columns of the census dataframe\n",
    "census_df.columns = [\"state\", \"sex\", \"age\", 'pop2000', 'pop2008']\n",
    "# append the data from census_df to the \"census\" table via connection\n",
    "census_df.to_sql(name=\"census\", con=connection, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Updating tables\n",
    "\n",
    "from sqlalchemy import update\n",
    "select_stmt = select([state_fact]).where(state_fact.columns.name == 'New York')\n",
    "results = connection.execute(select_stmt).fetchall()\n",
    "print(results[0]['fips_state'])\n",
    "update_stmt = update(state_fact).values(fips_state = 36)\n",
    "update_stmt = update_stmt.where(state_fact.columns.name == 'New York')\n",
    "update_results = connection.execute(update_stmt)\n",
    "# Execute select_stmt again and fetch the new results\n",
    "new_results = connection.execute(select_stmt).fetchall()\n",
    "print(new_results[0]['fips_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting tables\n",
    "\n",
    "from sqlalchemy import delete\n",
    "# Delete census table\n",
    "delete_stmt = delete(\"census\")\n",
    "results = connection.execute(delete_stmt)\n",
    "# Deleting selected data\n",
    "#Delete records from the census table where clause to target Men ('M') age 36\n",
    "delete_stmt = delete(census)\n",
    "delete_stmt = delete_stmt.where(\n",
    "    and_(census.columns.sex == 'M',\n",
    "         census.columns.age == 36))\n",
    "results = connection.execute(delete_stmt)\n",
    "# Drop the state_fact table\n",
    "state_fact.drop(engine)\n",
    "# Check to see if state_fact exists\n",
    "print(state_fact.exists(engine))\n",
    "# Drop all tables\n",
    "metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-18 11:24:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-09-18 11:24:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.1 (default, Dec 14 2018, 13:28:58) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.4.2, Platform Darwin-16.7.0-x86_64-i386-64bit\n",
      "2019-09-18 11:24:45 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-09-18 11:24:45 [scrapy.extensions.telnet] INFO: Telnet Password: e3418451e3c3a14b\n",
      "2019-09-18 11:24:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-09-18 11:24:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-09-18 11:24:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-09-18 11:24:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-09-18 11:24:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-09-18 11:24:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-09-18 11:24:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6033\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-d3b77a317f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVegan_Restaurant_Spider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \"\"\"\n\u001b[1;32m   1261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class Vegan_Restaurant_Spider(scrapy.Spider):\n",
    "  name = \"vegan_restaurant_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    urls = ['https://secretldn.com/vegan-restaurants-london/']\n",
    "    yield scrapy.Request(url = urls[0],\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "        for h3 in response.xpath('//div[@class = \"entry-content\"]//h3'):\n",
    "            re.append({'name': h3.css('a::text').extract_first(),\n",
    "                   'link': h3.xpath('./a/@href').extract_first(),\n",
    "                  'loc': h3.xpath('./text()').extract()})\n",
    "    \n",
    "re = []\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(Vegan_Restaurant_Spider)\n",
    "process.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-18 11:26:51 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
      "2019-09-18 11:26:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 126.251822,\n",
      " 'finish_reason': 'shutdown',\n",
      " 'finish_time': datetime.datetime(2019, 9, 18, 10, 26, 51, 615982),\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 69865472,\n",
      " 'memusage/startup': 69865472,\n",
      " 'start_time': datetime.datetime(2019, 9, 18, 10, 24, 45, 364160)}\n",
      "2019-09-18 11:26:51 [scrapy.core.engine] INFO: Spider closed (shutdown)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DeferredList at 0x112b2a080 current result: [(True, None)]>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.start()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-18 10:46:04 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): secretldn.com:443\n",
      "2019-09-18 10:46:04 [urllib3.connectionpool] DEBUG: https://secretldn.com:443 \"GET /vegan-restaurants-london/ HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "html = requests.get( 'https://secretldn.com/vegan-restaurants-london/').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "response = Selector( text = html )\n",
    "re = []\n",
    "for h3 in response.xpath('//div[@class = \"entry-content\"]//h3'):\n",
    "    re.append({'name': h3.css('a::text').extract_first(),\n",
    "                   'link': h3.xpath('./a/@href').extract_first(),\n",
    "                  'loc': h3.xpath('./text()').extract()})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "for h3 in response.xpath('//div[@class = \"entry-content\"]//h3'):\n",
    "    ans.append([\n",
    "                h3.css('a::text').extract_first(),\n",
    "                h3.xpath('./a/@href').extract_first(),\n",
    "                h3.xpath('./text()').extract()[1]\n",
    "            ])\n",
    "    for rest in ans:\n",
    "        scraped_info[rest[0]] = {\n",
    "                'name': rest[0],\n",
    "                'link': rest[1],\n",
    "                'location': rest[2]\n",
    "            }\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "h3 = response.xpath('//div[@class = \"entry-content\"]//h3')\n",
    "name = h3.css('a::text').extract()\n",
    "link = h3.xpath('./a/@href').extract()\n",
    "loc = h3.xpath('./text()').extract()\n",
    "c_l = []\n",
    "for lc in loc:\n",
    "    clean = re.search(\"[A-Za-z](.*)\",lc)\n",
    "    if clean != None:\n",
    "        c_l.append(clean.group(0))\n",
    "    \n",
    "row_data = zip(name[:-1], link[:-1], c_l[:-2])\n",
    "scraped_info = []\n",
    "for item in row_data:\n",
    "    scraped_info.append({\n",
    "                'name': item[0],\n",
    "                'link': item[1],\n",
    "                'location': item[2]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h3>1. <a href=\"https://feverup.com/m/75024?utm_source=secretldn&amp;utm_medium=post&amp;utm_campaign=75024_lon&amp;utm_content=vegan-restaurants-london\" onclick=\"return trackOutboundLink (\\'outbound_link\\', \\'cta1\\', \\' https://feverup.com/m/75024?utm_source=secretldn&amp;utm_medium=post&amp;utm_campaign=75024_lon&amp;utm_content=vegan-restaurants-london \\', \\'true\\')\" target=\"_blank\">Pied a Terre</a>,\\xa0Fitzrovia</h3>'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//div[@class = \"entry-content\"]//h3').extract()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('222 Veggie Vegan', 'http://www.222veggievegan.com', 'F'),\n",
       " ('Cafe Van Gogh', 'https://www.cafevangogh.co.uk', 'v'),\n",
       " ('Comptoir V', 'https://www.comptoirv.co.uk', 'i'),\n",
       " ('Genesis',\n",
       "  'https://secretldn.com/genesis-vegan-restaurant-shoreditch/',\n",
       "  'i'),\n",
       " ('Mildred’s', 'http://www.mildreds.co.uk', '\\xa0'),\n",
       " ('Mooshies', 'https://veganburger.org', 'r'),\n",
       " ('Pied a Terre',\n",
       "  'https://feverup.com/m/75024?utm_source=secretldn&utm_medium=post&utm_campaign=75024_lon&utm_content=vegan-restaurants-london',\n",
       "  ','),\n",
       " ('Rasa', 'http://www.rasarestaurants.com/index.html', 't'),\n",
       " ('SpiceBox', 'https://eatspicebox.co.uk/', 'a'),\n",
       " ('St Margaret’s House Gallery Cafe',\n",
       "  'http://www.stmargaretshouse.org.uk/gallerycafe/',\n",
       "  'o'),\n",
       " ('Wulf & Lamb', 'http://wulfandlamb.com', 'z')}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_data = zip(name, link, loc)\n",
    "a = set(row_data)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "c_l = []\n",
    "for lc in loc:\n",
    "    clean = re.search(\"[A-Za-z](.*)\",lc)\n",
    "    if clean != None:\n",
    "        c_l.append(clean.group(0))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(name[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 4), match='Ato'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search(\"[A-Z](.*)\",'gAto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Pied a Terre',\n",
       "  'link': 'https://feverup.com/m/75024?utm_source=secretldn&utm_medium=post&utm_campaign=75024_lon&utm_content=vegan-restaurants-london',\n",
       "  'location': 'Fitzrovia'},\n",
       " {'name': 'Mildred’s',\n",
       "  'link': 'http://www.mildreds.co.uk',\n",
       "  'location': 'various locations'},\n",
       " {'name': '222 Veggie Vegan',\n",
       "  'link': 'http://www.222veggievegan.com',\n",
       "  'location': 'West Kensington'},\n",
       " {'name': 'Comptoir V',\n",
       "  'link': 'https://www.comptoirv.co.uk',\n",
       "  'location': 'Kensal Green'},\n",
       " {'name': 'Rasa',\n",
       "  'link': 'http://www.rasarestaurants.com/index.html',\n",
       "  'location': 'Stoke Newington'},\n",
       " {'name': 'Wulf & Lamb',\n",
       "  'link': 'http://wulfandlamb.com',\n",
       "  'location': 'Chelsea'},\n",
       " {'name': 'Mooshies',\n",
       "  'link': 'https://veganburger.org',\n",
       "  'location': 'Shoreditch'},\n",
       " {'name': 'St Margaret’s House Gallery Cafe',\n",
       "  'link': 'http://www.stmargaretshouse.org.uk/gallerycafe/',\n",
       "  'location': 'Bethnal Green'},\n",
       " {'name': 'Cafe Van Gogh',\n",
       "  'link': 'https://www.cafevangogh.co.uk',\n",
       "  'location': 'Brixton'},\n",
       " {'name': 'Genesis',\n",
       "  'link': 'https://secretldn.com/genesis-vegan-restaurant-shoreditch/',\n",
       "  'location': 'Shoreditch'},\n",
       " {'name': 'SpiceBox',\n",
       "  'link': 'https://eatspicebox.co.uk/',\n",
       "  'location': 'Walthamstow'},\n",
       " {'name': 'Kalifornia Kitchen',\n",
       "  'link': 'https://secretldn.com/kalifornia-kitchen-vegan-restaurant/',\n",
       "  'location': 'Fitzrovia and Fulham'},\n",
       " {'name': 'WAVE',\n",
       "  'link': 'https://secretldn.com/we-are-vegan-everything/',\n",
       "  'location': 'Hackney'},\n",
       " {'name': 'Temple of Seitan',\n",
       "  'link': 'http://templeofseitan.co.uk',\n",
       "  'location': 'Hackney and Camden'},\n",
       " {'name': 'CookDaily',\n",
       "  'link': 'https://www.cookdaily.co.uk',\n",
       "  'location': 'London Fields'},\n",
       " {'name': 'Young Vegans',\n",
       "  'link': 'http://www.youngvegans.co.uk',\n",
       "  'location': 'Camden'},\n",
       " {'name': 'La Gelatiera',\n",
       "  'link': 'http://www.lagelatiera.co.uk/index.html',\n",
       "  'location': 'Covent Garden or Stratford'},\n",
       " {'name': 'by CHLOE.',\n",
       "  'link': 'https://secretldn.com/by-chloe-vegan-cafe/',\n",
       "  'location': 'various locations'},\n",
       " {'name': 'The Vurger Co',\n",
       "  'link': 'http://www.thevurgerco.com/',\n",
       "  'location': 'Shoreditch and Canary Wharf'},\n",
       " {'name': 'Sutton and Sons',\n",
       "  'link': 'https://secretldn.com/vegan-fish-chip-shop-hackney-london/',\n",
       "  'location': 'Hackney'},\n",
       " {'name': 'Biff’s Jack Shack',\n",
       "  'link': 'https://secretldn.com/biffs-jack-shack-vegan/',\n",
       "  'location': 'Shoreditch, Walthamstow, and Homerton'}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
